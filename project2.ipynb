{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9hZAb9TNPUJ"
      },
      "outputs": [],
      "source": [
        "# Packages\n",
        "import numpy as np\n",
        "import time\n",
        "from queue import Queue\n",
        "\n",
        "# Helper Variables\n",
        "allfeatures = [1,2,3,4] # only integers at the moment\n",
        "\n",
        "class node:\n",
        "  def __init__(self, _features:set = set(), score:float = 0.0, classification:float = 0.0):\n",
        "    self.classification = classification # float\n",
        "    self.features_set = _features # set\n",
        "    self.features = [] # list???\n",
        "    self.parents = [] # node\n",
        "    self.children = [] # nodes\n",
        "    self.score = 0.0 # float, ig\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.features_set)\n",
        "\n",
        "  def formattedStr(self):\n",
        "    return f\"Classification: {int(self.classification)} | Features: {str(self.features)}\"\n",
        "\n",
        "# this is the filler snippet, dont use it\n",
        "def evaluate(n : node):\n",
        "  n.score = round(np.random.rand() * 100, 1)\n",
        "  return n.score\n",
        "\n",
        "def eval(n:node, instances:list, validator:Validator, selected_features:list):\n",
        "\n",
        "  features = []\n",
        "\n",
        "  for item in n.features_set:\n",
        "    features.append(\"Feature \" + str(item))\n",
        "\n",
        "  # Get default rate\n",
        "  if(len(features) <= 0):\n",
        "    temp_dict = {}\n",
        "    for instance in instances:\n",
        "      if(instance.classification in temp_dict):\n",
        "        temp_dict[instance.classification] += 1\n",
        "      else:\n",
        "        temp_dict[instance.classification] = 1\n",
        "\n",
        "    most_Common_Class_Size = 0\n",
        "\n",
        "    for classification in temp_dict:\n",
        "      if temp_dict[classification] > most_Common_Class_Size:\n",
        "        most_Common_Class_Size = temp_dict[classification]\n",
        "    default_rate = most_Common_Class_Size / len(instances)\n",
        "\n",
        "    n.score = default_rate\n",
        "  # Use k-folds 1-NN for score.\n",
        "  else:\n",
        "    n.score = validator.validate(selected_features)\n",
        "  return n.score\n",
        "\n",
        "class tree:\n",
        "  def __init__(self, validator:Validator, selected_features:list, root:node = node(), tail:node = node()):\n",
        "    self.root = root\n",
        "    self.tail = tail\n",
        "    self.validator = validator\n",
        "    self.features = selected_features\n",
        "\n",
        "  def __str__(self):\n",
        "    returnstr = \"\"\n",
        "    queue1 = Queue()\n",
        "    queue2 = Queue()\n",
        "    queue1.put(self.root)\n",
        "    while(not queue1.empty()):\n",
        "      # handle dupes and add children to next batch\n",
        "      nodesToPrint = set()\n",
        "\n",
        "      # Print all nodes on current depth\n",
        "      while(not queue1.empty()):\n",
        "        curr = queue1.get()\n",
        "        for child in curr.children:\n",
        "          if tuple(child.features_set) not in nodesToPrint:\n",
        "            queue2.put(child)\n",
        "          nodesToPrint.add(tuple(child.features_set))\n",
        "        returnstr += str(curr.features_set) + \" \"\n",
        "\n",
        "      # prepare next batch of children\n",
        "      while(not queue2.empty()):\n",
        "        queue1.put(queue2.get())\n",
        "      returnstr += \"\\n\"\n",
        "    return returnstr\n",
        "\n",
        "  def fill(self, instances:list = list()):\n",
        "    queue1 = Queue()\n",
        "    queue2 = Queue()\n",
        "    queue1.put(self.root)\n",
        "\n",
        "    # Use 2 queues for bootleg breadth first search, recursion is a sign of weakness\n",
        "    while(not queue1.empty()):\n",
        "      currFeatureSet = set()\n",
        "      while(not queue1.empty()):\n",
        "        curr = queue1.get()\n",
        "\n",
        "        if instances == 0 :\n",
        "          evaluate(curr)\n",
        "        else:\n",
        "          eval(curr, instances, self.validator, self.features)\n",
        "\n",
        "        # If we have a node in the list that is all our entire feature set we stop\n",
        "        if(set(self.features).difference(curr.features_set) == set()):\n",
        "          self.tail = curr\n",
        "          return\n",
        "\n",
        "        # Use 2nd queue to iterate through parents in future\n",
        "        queue2.put(curr)\n",
        "\n",
        "        # Gather all possible features generated from parent nodes\n",
        "        for feature in self.features:\n",
        "          newfeatures = set()\n",
        "          for currentfeature in curr.features_set:\n",
        "            newfeatures = curr.features_set.copy()\n",
        "            newfeatures.add(currentfeature)\n",
        "          newfeatures.add(feature)\n",
        "\n",
        "          # Create a set of unique tuples\n",
        "          if(newfeatures > curr.features_set):\n",
        "            # print(f\"{newfeatures}: added {feature}\")\n",
        "            if(len(newfeatures) > 1):\n",
        "              currFeatureSet.add(tuple(newfeatures))\n",
        "            else:\n",
        "              currFeatureSet.update(newfeatures)\n",
        "\n",
        "      # create child nodes with all new feature possibilities given parent nodes\n",
        "      children = []\n",
        "      for feature in currFeatureSet:\n",
        "        if(type(feature) == int):\n",
        "          child = node({feature})\n",
        "        else:\n",
        "          child = node(set(feature))\n",
        "        children.append(child)\n",
        "        queue1.put(child)\n",
        "\n",
        "      ## if the child is a super set of the parent, assign child to that parent\n",
        "      while(not queue2.empty()):\n",
        "        curr = queue2.get()\n",
        "        # print(f\"\\nSetting child for {curr}\")\n",
        "        for child in children:\n",
        "          # if the parent node is a subset of a child node\n",
        "          if(curr.features_set < child.features_set):\n",
        "            child.parents.append(curr)\n",
        "            curr.children.append(child)\n",
        "  #------------------------------------ END FUNC\n",
        "\n",
        "  def forwardSearch(self, instances):\n",
        "    bestState = self.root         # will be returned at the end, hopefully isnt self.root\n",
        "    bestStateScore = 0\n",
        "\n",
        "    if instances == 0:\n",
        "      bestStateScore = evaluate(bestState)\n",
        "    else:\n",
        "      bestStateScore = eval(bestState, instances, self.validator, self.features)\n",
        "\n",
        "    children = bestState.children # start by seeding the 1-feature combination nodes\n",
        "\n",
        "    print(\"Running neareset neighbor with no features (default rate), using \\\"leaving-one-out\\\" evaluation, I get an accuracy of \" + str(bestStateScore) + \"%\\n\")\n",
        "\n",
        "    print(\"Beginning search\\n\")\n",
        "    for i in range(len(self.features)):  # we're only gonna be searching so many times cuz it's greedy\n",
        "      bestScoreAtDepth = -1\n",
        "      bestStateAtDepth = node()\n",
        "\n",
        "      for child in children:\n",
        "        score = 0\n",
        "        if instances == 0:\n",
        "          score = evaluate(child)\n",
        "        else:\n",
        "          score = eval(child, instances, self.validator, self.features)\n",
        "\n",
        "        print(\"\\tUsing feature(s) \", end = \"\")\n",
        "        print(child, end = \"\")\n",
        "        print(\" accuracy is \" + str(score) + \"%\")\n",
        "\n",
        "        if score > bestScoreAtDepth:\n",
        "          bestScoreAtDepth = score\n",
        "          bestStateAtDepth = child\n",
        "\n",
        "      # -----------------------\n",
        "      print()\n",
        "\n",
        "      if bestScoreAtDepth >= bestStateScore: # better than previous bestState\n",
        "        bestState = bestStateAtDepth\n",
        "        bestStateScore = bestScoreAtDepth\n",
        "        print(\"\\nFeature set \", end = \"\")\n",
        "        print(bestStateAtDepth, end = \"\")\n",
        "        print(\" was best, accuracy is \" + str(bestScoreAtDepth) + \"%\\n\")\n",
        "      else:\n",
        "        print(\"(Warning, Accuracy has decreased! Continuing search in case of local maxima)\")\n",
        "\n",
        "      children = bestStateAtDepth.children # update for next iteration\n",
        "    # --------------------------------------\n",
        "\n",
        "    print(\"Finished search, best feature subset is \", end = \"\")\n",
        "    print(bestState, end = \"\")\n",
        "    print(\", which has an accuracy of \" + str(bestStateScore) + \"%\")\n",
        "    return bestState\n",
        "  # ---------------------------------------------- END FUNC\n",
        "\n",
        "  def backwardSearch(self, instances):\n",
        "    bestState = self.tail\n",
        "    bestStateScore = 0\n",
        "    if instances == 0:\n",
        "      bestStateScore = evaluate(bestState)\n",
        "    else:\n",
        "      bestStateScore = eval(bestState, instances, self.validator, self.features)\n",
        "\n",
        "    parents = bestState.parents\n",
        "\n",
        "    print(\"All features, 'random' evaluation: \" + str(bestStateScore) + \"%\\n\")\n",
        "    print(\"Start search\\n\")\n",
        "    for i in range(len(self.features)):\n",
        "      # if best state is the tail node\n",
        "      bestScoreAtDepth = -1\n",
        "      bestStateAtDepth = node()\n",
        "\n",
        "      for parent in parents:\n",
        "        score = 0\n",
        "        if instances == 0:\n",
        "          score = evaluate(parent)\n",
        "        else:\n",
        "          score = eval(parent, instances, self.validator, self.features)\n",
        "\n",
        "        print(\"\\tUsing feature(s) \", end = \"\")\n",
        "        print(parent, end = \"\")\n",
        "        print(\" accuracy is \" + str(score) + \"%\")\n",
        "\n",
        "        if score > bestScoreAtDepth:\n",
        "          bestScoreAtDepth = score\n",
        "          bestStateAtDepth = parent\n",
        "      # -----------------------\n",
        "      print()\n",
        "\n",
        "      if bestScoreAtDepth >= bestStateScore: # better than previous bestState\n",
        "        bestState = bestStateAtDepth\n",
        "        bestStateScore = bestScoreAtDepth\n",
        "        print(\"Feature set \", end = \"\")\n",
        "        print(bestState, end = \"\")\n",
        "        print(\" was best, accuracy is \" + str(bestStateScore) + \"%\\n\")\n",
        "      else:\n",
        "        print(\"Warning, Accuracy has decreased!\\n\")\n",
        "\n",
        "      parents = bestStateAtDepth.parents # update for next iteration\n",
        "    # --------------------------------------\n",
        "\n",
        "    print(\"Finished search, best feature subset is \", end = \"\")\n",
        "    print(bestState, end = \"\")\n",
        "    print(\", which has an accuracy of \" + str(bestStateScore) + \"%\")\n",
        "    return bestState\n",
        "  # ---------------------------------------------- END FUNC\n",
        "\n",
        "  # explore all good nodes that are within epsilon range of the best node.\n",
        "  # Epsilon is a range(in percentage) of the best feature set to determine\n",
        "  # if a feature set is within worth exploring.\n",
        "\n",
        "  # epsilon = 0, forward greedy search\n",
        "  # epsilon = 100, Breadth First Search\n",
        "  def SpecialYumYum(self, epsilon:float = 5):\n",
        "    queue1 = Queue()\n",
        "    queue2 = Queue()\n",
        "    bestNode = self.root\n",
        "    queue1.put(bestNode)\n",
        "    visitedNodes = set()\n",
        "    print(f\"No features, 'random' evaluation: {bestNode.score}%\\n\")\n",
        "    print(\"Start search\\n\")\n",
        "    # Use 2 queues for bootleg breadth first search, recursion is a sign of weakness\n",
        "    while(not queue1.empty()):\n",
        "      bestNodeAtDepth = node()\n",
        "      while(not queue1.empty()):\n",
        "        newBestNodes = []\n",
        "        bestNodes = []\n",
        "        validNodes = []\n",
        "        curr = queue1.get()\n",
        "\n",
        "        # Conditional statements for loop\n",
        "        if len(curr.children) == 0:\n",
        "          break\n",
        "        if tuple(curr.features_set) in visitedNodes:\n",
        "          continue\n",
        "        visitedNodes.add(tuple(curr.features_set))\n",
        "\n",
        "        # If find child with best score at current depth, compare to the best node we have\n",
        "        print(f\"For {curr}, Score: {curr.score}%\\n\")\n",
        "        for child in curr.children:\n",
        "          if tuple(child.features_set) not in visitedNodes:\n",
        "            print(f\"\\tUsing feature(s) {child} accuracy is {child.score}%\")\n",
        "            if child.score > bestNode.score - epsilon:\n",
        "              validNodes.append(child)\n",
        "            if child.score > bestNodeAtDepth.score:\n",
        "              bestNodeAtDepth = child\n",
        "        print()\n",
        "\n",
        "        for child in validNodes:\n",
        "          # Create new best if current node is better than all nodes by more than epsilon\n",
        "          if child.score > bestNode.score:\n",
        "            print(f\"Feature set {child} is better than best node({bestNode}), new best accuracy is {child.score}%\")\n",
        "            bestNode = child\n",
        "            newBestNodes = [child]\n",
        "\n",
        "            # Empty out next best candidates in range epsilon of new best\n",
        "            while not queue2.empty():\n",
        "              queue2.get()\n",
        "\n",
        "            # Create the set\n",
        "            for n in bestNodes:\n",
        "              if n.score > bestNode.score - epsilon:\n",
        "                newBestNodes.append(n)\n",
        "            bestNodes = newBestNodes.copy()\n",
        "\n",
        "          # Add node to list of bestNodes at this depth\n",
        "          elif child.score > bestNode.score - epsilon:\n",
        "            print(f\"Feature set {child} is within epsilon of best node({bestNode}), accuracy is {child.score}%\")\n",
        "            bestNodes.append(child)\n",
        "\n",
        "          # Node not worth exploring\n",
        "          else:\n",
        "            print(f\"Feature set {child} is not within epsilon of best node({bestNode}), accuracy is {child.score}%\")\n",
        "\n",
        "        # Queue up next best nodes at the next depth, or if no node was worthwhile we just pick the best node at that depth\n",
        "        print(f\"\\nFeature set {bestNodeAtDepth} was the best node available, accuracy is {bestNodeAtDepth.score}%\")\n",
        "        if len(bestNodes) > 0:\n",
        "          for bnode in bestNodes:\n",
        "            queue2.put(bnode)\n",
        "        else:\n",
        "          queue2.put(bestNodeAtDepth)\n",
        "        print()\n",
        "\n",
        "      while(not queue2.empty()):\n",
        "        queue1.put(queue2.get())\n",
        "    print(f\"Finished search, best feature subset is {bestNode}, which has an accuracy of {bestNode.score}%\")\n",
        "    return bestNode\n",
        "  # ---------------------------------------------- END FUNC\n",
        "\n",
        "#-------------------------------------------------------------- END CLASS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import A\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "# Classifying Time\n",
        "class Classifier:\n",
        "  def __init__(self):\n",
        "    self.training_data = list()\n",
        "    self.test_data = list()\n",
        "    self.frame = pd.DataFrame()\n",
        "\n",
        "  def __str__(self):\n",
        "    returnstr = \"{:<145}{:<145}\\n\".format(\"Training\", \"Test\")\n",
        "    for i in range(max(len(self.training_data), len(self.test_data))):\n",
        "      if(i < len(self.training_data)):\n",
        "        traindata = self.training_data[i].formattedStr()\n",
        "      else:\n",
        "        traindata = ''\n",
        "\n",
        "      if(i < len(self.test_data)):\n",
        "        testdata = self.test_data[i].formattedStr()\n",
        "      else:\n",
        "        testdata = ''\n",
        "      returnstr += \"{:<145}{:<145}\\n\".format(traindata, testdata)\n",
        "    return returnstr\n",
        "\n",
        "  # Can input test with split(0 to 1) %, split = .25 means 25% of data is test data,\n",
        "  # 75% becomes training data. The test array will always be assigned starting from\n",
        "  # index 0 to the desired test data amount.\n",
        "\n",
        "  # train() will append all data to the existing training set\n",
        "  def train(self, data:list=[], split:float=0, testAmount:int=0, dataframe = None):\n",
        "    # If we decide to use a panda then assign it for future use, hope that user has understanding\n",
        "    if isinstance(dataframe, pd.DataFrame):\n",
        "      self.frame = dataframe\n",
        "      if(len(data) == 0):\n",
        "        raise Exception(\"Recieved DataFrame but no data. Set data = list(node)\")\n",
        "\n",
        "    if(testAmount > 0):\n",
        "      size = testAmount\n",
        "    else:\n",
        "      size = int(len(data) * split)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "      if(i < size):\n",
        "        self.test_data.append(data[i])\n",
        "      else:\n",
        "        self.training_data.append(data[i])\n",
        "\n",
        "  # reset the model\n",
        "  def reset(self):\n",
        "    self.training_data = []\n",
        "    self.test_data = []\n",
        "\n",
        "  # Euclidean distance between two instances\n",
        "  def Euclidean_Distance_For_Numbers(self, training_node:node, test_node:node):\n",
        "    sum = 0\n",
        "    for feature in range(len(training_node.features)):\n",
        "      difference = training_node.features[feature] - test_node.features[feature]\n",
        "      sum += difference * difference\n",
        "    return math.sqrt(sum)\n",
        "\n",
        "  # Euclidean distance using pandas dataframe and a list. Columns of list is same as columns of dataframe\n",
        "  def Euclidean_Distance_For_Pandas(self, test:pd.DataFrame, frame:pd.DataFrame):\n",
        "    # strip Class Information for euclidean\n",
        "    features = []\n",
        "    for feature in frame:\n",
        "      if feature != \"Class Label\":\n",
        "        features.append(feature)\n",
        "\n",
        "    a = test[features].values.squeeze()\n",
        "    b = frame[features]\n",
        "    difference = b - a\n",
        "    square = difference * difference\n",
        "    sum = np.sum(square,axis=1)\n",
        "    return np.sqrt(sum)\n",
        "\n",
        "  # The test function can take in a list of test data and return the closest instance\n",
        "  # to each given test instance\n",
        "  def test(self, testdata = None):\n",
        "    start_time = time.time()\n",
        "    returnClassification = []\n",
        "    formatstr = \"{:<15}{:<15}\"\n",
        "    # print(formatstr.format(\"Test Instance\", \"Classifying Instance\"))\n",
        "\n",
        "    # Test against the test set that was last used/currently stored\n",
        "    if testdata is None:\n",
        "      for test_instance in self.test_data:\n",
        "        nearestNeighbor = math.inf\n",
        "        bestClassifier = node()\n",
        "        for classifying_instance in self.training_data:\n",
        "          distance = self.Euclidean_Distance_For_Numbers(classifying_instance, test_instance)\n",
        "          if distance < nearestNeighbor:\n",
        "            bestClassifier = classifying_instance\n",
        "            nearestNeighbor = distance\n",
        "        print(formatstr.format(test_instance.classification, bestClassifier.classification))\n",
        "        returnClassification.append(bestClassifier)\n",
        "\n",
        "    # If a list is given test each instance in the list\n",
        "    elif isinstance(testdata, list):\n",
        "      self.test_data = testdata.copy()\n",
        "      for test_instance in self.test_data:\n",
        "        nearestNeighbor = math.inf\n",
        "        bestClassifier = node()\n",
        "        for classifying_instance in self.training_data:\n",
        "          distance = self.Euclidean_Distance_For_Numbers(classifying_instance, test_instance)\n",
        "          if distance < nearestNeighbor:\n",
        "            bestClassifier = classifying_instance\n",
        "            nearestNeighbor = distance\n",
        "        print(formatstr.format(test_instance.classification, bestClassifier.classification))\n",
        "        returnClassification.append(bestClassifier)\n",
        "\n",
        "    # Use Pandas dataframe\n",
        "    elif isinstance(testdata, pd.DataFrame):\n",
        "      distances = self.Euclidean_Distance_For_Pandas(testdata, self.frame)\n",
        "      # print(formatstr.format(testdata[\"Class Label\"].to_list()[0], int(self.training_data[distances.argmin()].classification)))\n",
        "      returnClassification.append(self.training_data[distances.idxmin()])\n",
        "\n",
        "    # If given a single node\n",
        "    else:\n",
        "      nearestNeighbor = math.inf\n",
        "      bestClassifier = node()\n",
        "      for classifying_instance in self.training_data:\n",
        "        distance = self.Euclidean_Distance_For_Numbers(classifying_instance, testdata)\n",
        "        if distance < nearestNeighbor:\n",
        "          bestClassifier = classifying_instance\n",
        "          nearestNeighbor = distance\n",
        "      print(formatstr.format(testdata.classification, bestClassifier.classification))\n",
        "      returnClassification.append(bestClassifier)\n",
        "    end_time = time.time()\n",
        "    # print(f\"Time elapsed: {end_time - start_time}\\n\")\n",
        "    return returnClassification"
      ],
      "metadata": {
        "id": "A-BPkVh6rovT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "class Validator:\n",
        "  def __init__(self, classifier:Classifier, features:set, instances:list[node]):\n",
        "    self.classifier = classifier\n",
        "    self.features = list(features)\n",
        "    self.instances = instances\n",
        "    self.data = []\n",
        "\n",
        "    for instance in instances:\n",
        "      self.data.append(instance.features)\n",
        "    self.data = np.array(self.data)\n",
        "\n",
        "    self.frame = pd.DataFrame({\"Class Label\": [int(i.classification) for i in instances]})\n",
        "    for feature in range(len(instances[0].features)):\n",
        "      feature_frame = pd.DataFrame({\"Feature \" + str(feature + 1): self.data[:,feature]})\n",
        "      self.frame = pd.concat([self.frame, feature_frame], axis = 1)\n",
        "\n",
        "    print(\"Please wait while I normalize the data...  \", end=\"\")\n",
        "    scaler = StandardScaler()\n",
        "    columns_to_standardize = self.frame.columns.difference(['Class Label'])\n",
        "    self.frame[columns_to_standardize] = (self.frame[columns_to_standardize] - self.frame[columns_to_standardize].mean()) / self.frame[columns_to_standardize].std()\n",
        "    print(\"Done!\\n\")\n",
        "  # -------------------\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.frame)\n",
        "  # -------------------\n",
        "\n",
        "  # Given a set of features, k-folds cross validation on feature set. Refer to input code further below for usage.\n",
        "  def validate(self, selected_features:list):\n",
        "    validation_table = []\n",
        "\n",
        "    features = []\n",
        "    for item in selected_features:\n",
        "      features.append(\"Feature \" + str(item))\n",
        "    # train model with features\n",
        "    selected_frame = self.frame[[\"Class Label\"] + features]\n",
        "\n",
        "    for i in range(len(self.instances)):\n",
        "      # target specific instance\n",
        "      test_instance_mask = selected_frame.index.isin([i])\n",
        "\n",
        "      # Get instance we for test set, I use a panda frame since its easier to do large amounts of data\n",
        "      test_instance = selected_frame[test_instance_mask]\n",
        "      frame_masked = selected_frame[~test_instance_mask]\n",
        "\n",
        "      # train and test\n",
        "      self.classifier.train(data = self.instances, dataframe = frame_masked)\n",
        "      classifierList = self.classifier.test(test_instance)\n",
        "\n",
        "      # technically just 1 instance, but still use loop to validate. Add validation to table.\n",
        "      for classified_instance in classifierList:\n",
        "        validation_table.append(classified_instance.classification == self.instances[i].classification)\n",
        "\n",
        "    # Now we have a filled valdation table of true and false(correct or incorrect classification)\n",
        "    return sum(validation_table) / len(validation_table)"
      ],
      "metadata": {
        "id": "qYuZrEtIOI3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading Text Time\n",
        "small_file = list()\n",
        "large_file = list()\n",
        "small = r\"small-test-dataset.txt\"\n",
        "large = r\"large-test-dataset.txt\"\n",
        "with open(small, \"r\") as f:\n",
        "  for line in f:\n",
        "    small_file.append(line.split())\n",
        "  f.close()\n",
        "\n",
        "with open(large, \"r\") as f:\n",
        "  for line in f:\n",
        "    large_file.append(line.split())\n",
        "  f.close()\n",
        "\n",
        "# Conversion Time part 1\n",
        "small_contents = []\n",
        "for instance in small_file:\n",
        "  small_contents.append(list(map(float, instance)))\n",
        "\n",
        "large_contents = []\n",
        "for instance in large_file:\n",
        "  large_contents.append(list(map(float, instance)))\n",
        "\n",
        "# Conversion Time Part 2\n",
        "## ------------------------------------------------------------------------------------------------------------------------------------\n",
        "## MASSIVE NOTE: if you notice something is wrong with the data re run the data collection starting from when we read in the file. It is just a collab thing.\n",
        "##                - Classification should be either a 1 or 2 for example\n",
        "## ------------------------------------------------------------------------------------------------------------------------------------\n",
        "small_instances = []\n",
        "for instance in small_contents:\n",
        "  n = node()\n",
        "  n.classification = instance.pop(0)\n",
        "  n.features = instance.copy()\n",
        "  n.features_set = set(n.features)\n",
        "  small_instances.append(n)\n",
        "\n",
        "large_instances = []\n",
        "for instance in large_contents:\n",
        "  n = node()\n",
        "  n.classification = instance.pop(0)\n",
        "  n.features = instance.copy()\n",
        "  n.features_set = set(n.features)\n",
        "  large_instances.append(n)\n",
        "\n",
        "smallClassifier = Classifier()\n",
        "\n",
        "print(f\"Small Dataset: small-test-dataset.txt (Has {len(small_instances)} instances and {len(small_instances[0].features)} features)\")\n",
        "userInput = input(f\"Enter the desired features (1 - {len(small_instances[0].features)}), separated by spaces: \")\n",
        "print(\"\\r\")\n",
        "userInputAsList = userInput.split()\n",
        "\n",
        "# Use set since we dont want to look at duplicate features.\n",
        "features = set()\n",
        "for item in userInputAsList:\n",
        "  features.add(item)\n",
        "\n",
        "smallValidator = Validator(smallClassifier, features, small_instances)\n",
        "\n",
        "print(\"Data Frame for small data set:\\n\")\n",
        "print(smallValidator)\n",
        "\n",
        "small_accuracy = smallValidator.validate(selected_features = list(features))\n",
        "print(f\"\\t- If you use only {[int(feature) for feature in userInputAsList]}, accuracy should be about {small_accuracy}\")\n",
        "\n",
        "largeClassifier = Classifier()\n",
        "\n",
        "print(\"Data Frame for large data set:\\n\")\n",
        "\n",
        "print(f\"Large Dataset: large-test-dataset.txt (Has {len(large_instances)} instances and {len(large_instances[0].features)} features)\")\n",
        "userInput = input(f\"Enter the desired features (1 - {len(large_instances[0].features)}), separated by spaces: \")\n",
        "print(\"\\r\")\n",
        "userInputAsList = userInput.split()\n",
        "\n",
        "# Use set since we dont want to look at duplicate features.\n",
        "features = set()\n",
        "for item in userInputAsList:\n",
        "  features.add(item)\n",
        "\n",
        "largeValidator = Validator(largeClassifier, features, large_instances)\n",
        "print(largeValidator)\n",
        "large_accuracy = largeValidator.validate(selected_features = list(features))\n",
        "print(f\"\\t- If you use only {[int(feature) for feature in userInputAsList]}, accuracy should be about {large_accuracy}\")"
      ],
      "metadata": {
        "id": "OLRkLMY85_GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in data --------------------------------------------\n",
        "small_file = list()\n",
        "large_file = list()\n",
        "small = r\"CS170_Spring_2024_Small_data__29.txt\"\n",
        "large = r\"CS170_Spring_2024_Large_data__29.txt\"\n",
        "\n",
        "with open(small, \"r\") as f:\n",
        "  for line in f:\n",
        "    small_file.append(line.split())\n",
        "  f.close()\n",
        "\n",
        "with open(large, \"r\") as f:\n",
        "  for line in f:\n",
        "    large_file.append(line.split())\n",
        "  f.close()\n",
        "\n",
        "# convert classification -----------------------------------\n",
        "small_contents = []\n",
        "for instance in small_file:\n",
        "  small_contents.append(list(map(float, instance)))\n",
        "\n",
        "large_contents = []\n",
        "for instance in large_file:\n",
        "  large_contents.append(list(map(float, instance)))\n",
        "print()\n",
        "\n",
        "# convert features -----------------------------------------\n",
        "small_instances = []\n",
        "for instance in small_contents:\n",
        "  n = node()\n",
        "  n.classification = instance.pop(0)\n",
        "  n.features = instance.copy()\n",
        "  n.features_set = set(n.features)\n",
        "  small_instances.append(n)\n",
        "\n",
        "large_instances = []\n",
        "for instance in large_contents:\n",
        "  n = node()\n",
        "  n.classification = instance.pop(0)\n",
        "  n.features = instance.copy()\n",
        "  n.features_set = set(n.features)\n",
        "  large_instances.append(n)\n",
        "\n",
        "# actually test ----------------------------------------------------------------\n",
        "dataset = input(\"Test small or large dataset? (1 or 2): \")\n",
        "algo = input(\"Which algo?\\n1 - Forward Search\\n2 - Backward Search\\n3 - Justin's Special\\n\")\n",
        "\n",
        "theInstances = []\n",
        "features = []\n",
        "\n",
        "if dataset == \"1\":\n",
        "  # get desired features\n",
        "  print(f\"Small Dataset (has {len(small_instances)} instances and {len(small_instances[0].features)} features)\")\n",
        "  features = input(f\"Enter the desired features (1 - {len(small_instances[0].features)}), separated by spaces. Enter nothing to use all features: \")\n",
        "  # prep tree construction\n",
        "  numFeatures = len(small_instances[0].features)\n",
        "  for i in range(int(numFeatures)):\n",
        "    allfeatures.append(i + 1)\n",
        "\n",
        "  theInstances = small_instances\n",
        "elif dataset == \"2\":\n",
        "  # get desired features\n",
        "  print(f\"Large Dataset (has {len(large_instances)} instances and {len(large_instances[0].features)} features)\")\n",
        "  features = input(f\"Enter the desired features (1 - {len(small_instances[0].features)}), separated by spaces. Enter nothing to use all features: \")\n",
        "  # prep tree construction\n",
        "  numFeatures = len(large_instances[0].features)\n",
        "  for i in range(int(numFeatures)):\n",
        "    allfeatures.append(i + 1)\n",
        "\n",
        "  theInstances = large_instances\n",
        "else:\n",
        "  print(\"Unrecognized input for dataset!\")\n",
        "  quit()\n",
        "classifier = Classifier()\n",
        "validator = Validator(classifier, set(allfeatures), theInstances)\n",
        "\n",
        "selected_features = []\n",
        "t1 = None\n",
        "if len(features) > 0:\n",
        "  t1 = tree(validator, features)\n",
        "else:\n",
        "  t1 = tree(validator, allfeatures)\n",
        "# construct the tree\n",
        "t1.fill(theInstances)\n",
        "stateFound = node()\n",
        "\n",
        "if algo == \"1\":\n",
        "  stateFound = t1.forwardSearch(theInstances)\n",
        "elif algo == \"2\":\n",
        "  stateFound = t1.backwardSearch(theInstances)\n",
        "elif algo == \"3\":\n",
        "  print(\"Justin's Special requires an epsilon that is used to explore nodes that are within range of the best state explored. 0 means Forward Selection, 100 means search entire tree\")\n",
        "  epsilon = input(\"Provide epsilon(Percentage range(ex. 5, 10, 20, 50)): \")\n",
        "  print()\n",
        "  stateFound = t1.SpecialYumYum(float(epsilon))\n",
        "else:\n",
        "  print(\"Unrecognized input for algorithm!\")\n",
        "  quit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "u4QlFyeb-det",
        "outputId": "c09deb05-dffb-44c4-bcd1-451040bde380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test small or large dataset? (1 or 2): 1\n",
            "Which algo?\n",
            "1 - Forward Search\n",
            "2 - Backward Search\n",
            "3 - Justin's Special\n",
            "1\n",
            "Small Dataset (has 100 instances and 10 features)\n",
            "Enter the desired features (1 - 10), separated by spaces. Enter nothing to use all features: \n",
            "Please wait while I normalize the data...  Done!\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f59424b129f1>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# construct the tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheInstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mstateFound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-7463b517f12c>\u001b[0m in \u001b[0;36mfill\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    111\u001b[0m           \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m           \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# If we have a node in the list that is all our entire feature set we stop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-7463b517f12c>\u001b[0m in \u001b[0;36meval\u001b[0;34m(n, instances, validator, selected_features)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;31m# Use k-folds 1-NN for score.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-26106c0f62b7>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, selected_features)\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;31m# train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_masked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m       \u001b[0mclassifierList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0;31m# technically just 1 instance, but still use loop to validate. Add validation to table.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-033d485ec2ea>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, testdata)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# Use Pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m       \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEuclidean_Distance_For_Pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m       \u001b[0;31m# print(formatstr.format(testdata[\"Class Label\"].to_list()[0], int(self.training_data[distances.argmin()].classification)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m       \u001b[0mreturnClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-033d485ec2ea>\u001b[0m in \u001b[0;36mEuclidean_Distance_For_Pandas\u001b[0;34m(self, test, frame)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0msquare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdifference\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdifference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__sub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rsub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   7455\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_method_FRAME\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7457\u001b[0;31m         \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_frame_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_dispatch_frame_op\u001b[0;34m(self, right, func, axis)\u001b[0m\n\u001b[1;32m   7460\u001b[0m     \u001b[0m_logical_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_arith_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7462\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_dispatch_frame_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7463\u001b[0m         \"\"\"\n\u001b[1;32m   7464\u001b[0m         \u001b[0mEvaluate\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mframe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mby\u001b[0m \u001b[0mevaluating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}